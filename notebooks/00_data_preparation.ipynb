{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# ğŸ¬ Analyse IMDb - PrÃ©paration des donnÃ©es\n",
    "\n",
    "**Objectif** : CrÃ©er un dataset propre avec films, notes, genres et acteurs/rÃ©alisateurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Configuration\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup_paths",
   "metadata": {},
   "source": [
    "## ğŸ“ Configuration des chemins (relatifs pour GitHub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "paths",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chemins relatifs (compatible GitHub)\n",
    "PROJECT_ROOT = Path.cwd().parent if 'notebooks' in str(Path.cwd()) else Path.cwd()\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "PARQUET_DIR = DATA_DIR / \"PARQUETS\"\n",
    "\n",
    "# CrÃ©er les dossiers\n",
    "PARQUET_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"âœ… Dossier crÃ©Ã© : {PARQUET_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step1",
   "metadata": {},
   "source": [
    "## ğŸ“¥ Ã‰TAPE 1 : Charger et filtrer title.basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_basics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL IMDb\n",
    "url_basics = \"https://datasets.imdbws.com/title.basics.tsv.gz\"\n",
    "\n",
    "# Chemin de sauvegarde\n",
    "parquet_basics = PARQUET_DIR / \"title_basics_movies.parquet\"\n",
    "\n",
    "# ParamÃ¨tres\n",
    "chunk_size = 500_000\n",
    "colonnes_utiles = [\n",
    "    'tconst', 'titleType', 'primaryTitle', 'originalTitle',\n",
    "    'isAdult', 'startYear', 'runtimeMinutes', 'genres'\n",
    "]\n",
    "\n",
    "# Lecture par chunks\n",
    "chunks_filtres = []\n",
    "print(\"ğŸ”„ Traitement de title.basics...\")\n",
    "\n",
    "for i, chunk in enumerate(pd.read_csv(\n",
    "    url_basics,\n",
    "    sep='\\t',\n",
    "    chunksize=chunk_size,\n",
    "    usecols=colonnes_utiles,\n",
    "    dtype={'startYear': str, 'runtimeMinutes': str, 'isAdult': str},\n",
    "    na_values='\\\\N'\n",
    "), 1):\n",
    "    \n",
    "    # Convertir isAdult en numÃ©rique (0 ou 1)\n",
    "    chunk['isAdult'] = pd.to_numeric(chunk['isAdult'], errors='coerce').fillna(1).astype(int)\n",
    "    \n",
    "    # âœ… FILTRES COMPLETS\n",
    "    chunk_filtre = chunk[\n",
    "        (chunk['titleType'] == 'movie') &\n",
    "        (chunk['isAdult'] == 0) &  # â† IMPORTANT : Exclure films adultes (0 = non-adulte)\n",
    "        (chunk['startYear'].notna())\n",
    "    ].copy()\n",
    "    \n",
    "    # Conversions\n",
    "    chunk_filtre['startYear'] = pd.to_numeric(chunk_filtre['startYear'], errors='coerce')\n",
    "    chunk_filtre['runtimeMinutes'] = pd.to_numeric(chunk_filtre['runtimeMinutes'], errors='coerce')\n",
    "    \n",
    "    # Filtre sur les annÃ©es (aprÃ¨s 1970)\n",
    "    chunk_filtre = chunk_filtre[chunk_filtre['startYear'] > 1970]\n",
    "    \n",
    "    chunks_filtres.append(chunk_filtre)\n",
    "    print(f\"  Chunk {i} : {len(chunk_filtre):,} films gardÃ©s\")\n",
    "\n",
    "# ConcatÃ©nation\n",
    "df_movies = pd.concat(chunks_filtres, ignore_index=True)\n",
    "print(f\"\\nâœ… Total : {len(df_movies):,} films conservÃ©s\")\n",
    "print(f\"ğŸ“Š PÃ©riode : {df_movies['startYear'].min():.0f} - {df_movies['startYear'].max():.0f}\")\n",
    "\n",
    "# Sauvegarde\n",
    "df_movies.to_parquet(parquet_basics, engine='pyarrow', compression='snappy', index=False)\n",
    "print(f\"ğŸ’¾ SauvegardÃ© : {parquet_basics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step2",
   "metadata": {},
   "source": [
    "## ğŸ“¥ Ã‰TAPE 2 : Charger title.ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_ratings",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL IMDb\n",
    "url_ratings = \"https://datasets.imdbws.com/title.ratings.tsv.gz\"\n",
    "parquet_ratings = PARQUET_DIR / \"title_ratings.parquet\"\n",
    "\n",
    "print(\"ğŸ”„ Traitement de title.ratings...\")\n",
    "\n",
    "# Lecture\n",
    "df_ratings = pd.read_csv(url_ratings, sep='\\t', na_values='\\\\N')\n",
    "\n",
    "# Filtrer pour garder seulement nos films\n",
    "tconst_movies = set(df_movies['tconst'])\n",
    "df_ratings = df_ratings[df_ratings['tconst'].isin(tconst_movies)]\n",
    "\n",
    "print(f\"âœ… {len(df_ratings):,} ratings conservÃ©s\")\n",
    "\n",
    "# Sauvegarde\n",
    "df_ratings.to_parquet(parquet_ratings, engine='pyarrow', compression='snappy', index=False)\n",
    "print(f\"ğŸ’¾ SauvegardÃ© : {parquet_ratings}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step3",
   "metadata": {},
   "source": [
    "## ğŸ‡«ğŸ‡· Ã‰TAPE 3 : Charger les titres rÃ©gionaux (France)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_akas",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL IMDb\n",
    "url_akas = \"https://datasets.imdbws.com/title.akas.tsv.gz\"\n",
    "parquet_akas = PARQUET_DIR / \"title_akas_fr.parquet\"\n",
    "\n",
    "print(\"ğŸ”„ Traitement de title.akas (titres rÃ©gionaux)...\")\n",
    "\n",
    "# ParamÃ¨tres\n",
    "chunks_filtres = []\n",
    "chunk_size = 500_000\n",
    "\n",
    "for i, chunk in enumerate(pd.read_csv(\n",
    "    url_akas,\n",
    "    sep='\\t',\n",
    "    chunksize=chunk_size,\n",
    "    usecols=['titleId', 'title', 'region', 'language', 'types', 'isOriginalTitle'],\n",
    "    na_values='\\\\N'\n",
    "), 1):\n",
    "    \n",
    "    # Filtrer uniquement les films qu'on a + rÃ©gion FR\n",
    "    chunk_filtre = chunk[\n",
    "        (chunk['titleId'].isin(tconst_movies)) &\n",
    "        (chunk['region'] == 'FR')\n",
    "    ].copy()\n",
    "    \n",
    "    chunks_filtres.append(chunk_filtre)\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        print(f\"  {i} chunks traitÃ©s\")\n",
    "\n",
    "df_akas = pd.concat(chunks_filtres, ignore_index=True)\n",
    "print(f\"\\nâœ… {len(df_akas):,} titres franÃ§ais trouvÃ©s\")\n",
    "\n",
    "# Renommer pour clartÃ©\n",
    "df_akas = df_akas.rename(columns={'titleId': 'tconst', 'title': 'frenchTitle'})\n",
    "\n",
    "# Garder le meilleur titre FR par film (prioritÃ© au titre original si dispo en FR)\n",
    "df_akas['priority'] = df_akas['isOriginalTitle'].fillna(0).astype(int)\n",
    "df_akas = df_akas.sort_values('priority', ascending=False)\n",
    "df_akas = df_akas.drop_duplicates(subset='tconst', keep='first')\n",
    "df_akas = df_akas[['tconst', 'frenchTitle', 'region']]\n",
    "\n",
    "print(f\"ğŸ“Š {len(df_akas):,} films avec titre franÃ§ais unique\")\n",
    "\n",
    "# Sauvegarde\n",
    "df_akas.to_parquet(parquet_akas, engine='pyarrow', compression='snappy', index=False)\n",
    "print(f\"ğŸ’¾ SauvegardÃ© : {parquet_akas}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step4",
   "metadata": {},
   "source": [
    "## ğŸ”— Ã‰TAPE 4 : CrÃ©er le dataset complet avec genres encodÃ©s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "merge_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jointure movies + ratings + titres franÃ§ais\n",
    "print(\"ğŸ”— Jointure movies + ratings + titres franÃ§ais...\")\n",
    "df_complet = df_movies.merge(df_ratings, on='tconst', how='left')\n",
    "df_complet = df_complet.merge(df_akas, on='tconst', how='left')\n",
    "\n",
    "print(f\"âœ… Dataset complet : {len(df_complet):,} films\")\n",
    "print(f\"ğŸ“Š Films avec notes : {df_complet['averageRating'].notna().sum():,}\")\n",
    "print(f\"ğŸ‡«ğŸ‡· Films avec titre franÃ§ais : {df_complet['frenchTitle'].notna().sum():,}\")\n",
    "\n",
    "# Encoder les genres\n",
    "print(\"\\nğŸ­ Encodage des genres...\")\n",
    "df_complet['genres'] = df_complet['genres'].fillna('')\n",
    "df_complet = pd.concat([\n",
    "    df_complet,\n",
    "    df_complet['genres'].str.get_dummies(sep=',')\n",
    "], axis=1)\n",
    "\n",
    "genre_cols = [col for col in df_complet.columns if col not in df_movies.columns and col not in df_ratings.columns and col not in df_akas.columns]\n",
    "print(f\"âœ… {len(genre_cols)} genres encodÃ©s\")\n",
    "\n",
    "# AperÃ§u\n",
    "print(\"\\nğŸ” AperÃ§u :\")\n",
    "display(df_complet[['primaryTitle', 'frenchTitle', 'startYear', 'genres', 'averageRating']].head())\n",
    "\n",
    "# Sauvegarde intermÃ©diaire\n",
    "parquet_complet = PARQUET_DIR / \"imdb_complet_base.parquet\"\n",
    "df_complet.to_parquet(parquet_complet, engine='pyarrow', compression='snappy', index=False)\n",
    "print(f\"ğŸ’¾ SauvegardÃ© : {parquet_complet}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step5",
   "metadata": {},
   "source": [
    "## ğŸ­ Ã‰TAPE 5 : Ajouter les acteurs et rÃ©alisateurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_principals",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL IMDb\n",
    "url_principals = \"https://datasets.imdbws.com/title.principals.tsv.gz\"\n",
    "\n",
    "# ParamÃ¨tres\n",
    "chunks_filtres = []\n",
    "chunk_size = 500_000\n",
    "\n",
    "print(\"ğŸ”„ Traitement de title.principals (peut prendre 10 min)...\")\n",
    "\n",
    "for i, chunk in enumerate(pd.read_csv(\n",
    "    url_principals,\n",
    "    sep='\\t',\n",
    "    chunksize=chunk_size,\n",
    "    usecols=['tconst', 'ordering', 'nconst', 'category'],\n",
    "    na_values='\\\\N'\n",
    "), 1):\n",
    "    \n",
    "    # Filtre : Top 5 acteurs + tous les rÃ©alisateurs\n",
    "    chunk_filtre = chunk[\n",
    "        (chunk['tconst'].isin(tconst_movies)) &\n",
    "        (\n",
    "            ((chunk['category'].isin(['actor', 'actress'])) & (chunk['ordering'] <= 5)) |\n",
    "            (chunk['category'] == 'director')\n",
    "        )\n",
    "    ].copy()\n",
    "    \n",
    "    chunks_filtres.append(chunk_filtre)\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        print(f\"  {i} chunks traitÃ©s\")\n",
    "\n",
    "df_principals = pd.concat(chunks_filtres, ignore_index=True)\n",
    "print(f\"\\nâœ… {len(df_principals):,} lignes conservÃ©es\")\n",
    "\n",
    "# Sauvegarder\n",
    "parquet_principals = PARQUET_DIR / \"title_principals_filtered.parquet\"\n",
    "df_principals.to_parquet(parquet_principals, engine='pyarrow', compression='snappy', index=False)\n",
    "print(f\"ğŸ’¾ SauvegardÃ© : {parquet_principals}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_names_optimized",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âš¡ CHARGEMENT OPTIMISÃ‰ de name.basics\n",
    "# On charge SEULEMENT les personnes prÃ©sentes dans nos films (50-100K au lieu de 13M+)\n",
    "\n",
    "url_names = \"https://datasets.imdbws.com/name.basics.tsv.gz\"\n",
    "\n",
    "# Extraire la liste des nconst nÃ©cessaires depuis df_principals\n",
    "nconst_needed = set(df_principals['nconst'].unique())\n",
    "print(f\"ğŸ¯ Personnes Ã  rechercher : {len(nconst_needed):,}\")\n",
    "\n",
    "print(\"\\nğŸ”„ Chargement OPTIMISÃ‰ de name.basics (filtrage immÃ©diat)...\")\n",
    "print(\"   Cette Ã©tape peut prendre 3-5 minutes...\")\n",
    "\n",
    "chunks_names = []\n",
    "chunk_size = 500_000\n",
    "total_found = 0\n",
    "\n",
    "for i, chunk in enumerate(pd.read_csv(\n",
    "    url_names,\n",
    "    sep='\\t',\n",
    "    chunksize=chunk_size,\n",
    "    usecols=['nconst', 'primaryName'],\n",
    "    na_values='\\\\N'\n",
    "), 1):\n",
    "    # âš¡ FILTRAGE IMMÃ‰DIAT : Ne garder QUE les personnes nÃ©cessaires\n",
    "    chunk_filtre = chunk[chunk['nconst'].isin(nconst_needed)]\n",
    "    \n",
    "    if len(chunk_filtre) > 0:\n",
    "        chunks_names.append(chunk_filtre)\n",
    "        total_found += len(chunk_filtre)\n",
    "    \n",
    "    # Affichage progression tous les 10 chunks\n",
    "    if i % 10 == 0:\n",
    "        print(f\"  {i} chunks traitÃ©s | {total_found:,} noms trouvÃ©s\")\n",
    "    \n",
    "    # âš¡ OPTIMISATION : ArrÃªter si on a trouvÃ© tout le monde\n",
    "    if total_found >= len(nconst_needed):\n",
    "        print(f\"  âœ… Tous les noms trouvÃ©s aprÃ¨s {i} chunks !\")\n",
    "        break\n",
    "\n",
    "# ConcatÃ©nation\n",
    "df_names = pd.concat(chunks_names, ignore_index=True)\n",
    "\n",
    "# Supprimer les doublons (au cas oÃ¹)\n",
    "df_names = df_names.drop_duplicates(subset='nconst')\n",
    "\n",
    "print(f\"\\nâœ… {len(df_names):,} noms rÃ©cupÃ©rÃ©s sur {len(nconst_needed):,} recherchÃ©s\")\n",
    "print(f\"ğŸ“Š Taux de couverture : {len(df_names) / len(nconst_needed) * 100:.1f}%\")\n",
    "\n",
    "# Sauvegarder (optionnel mais utile pour dÃ©bug)\n",
    "parquet_names = PARQUET_DIR / \"name_basics_filtered.parquet\"\n",
    "df_names.to_parquet(parquet_names, engine='pyarrow', compression='snappy', index=False)\n",
    "print(f\"ğŸ’¾ SauvegardÃ© : {parquet_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "merge_cast",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… FUSION avec les noms complets\n",
    "\n",
    "print(\"ğŸ”— Jointure principals + names...\")\n",
    "df_principals_noms = df_principals.merge(df_names, on='nconst', how='left')\n",
    "\n",
    "# CrÃ©er les listes d'acteurs par film (avec NOMS COMPLETS)\n",
    "acteurs = df_principals_noms[\n",
    "    df_principals_noms['category'].isin(['actor', 'actress'])\n",
    "].groupby('tconst')['primaryName'].apply(list).reset_index(name='acteurs')\n",
    "\n",
    "# CrÃ©er les listes de rÃ©alisateurs par film (avec NOMS COMPLETS)\n",
    "realisateurs = df_principals_noms[\n",
    "    df_principals_noms['category'] == 'director'\n",
    "].groupby('tconst')['primaryName'].apply(list).reset_index(name='realisateurs')\n",
    "\n",
    "print(f\"âœ… {len(acteurs):,} films avec acteurs\")\n",
    "print(f\"âœ… {len(realisateurs):,} films avec rÃ©alisateurs\")\n",
    "\n",
    "# Joindre Ã  df_complet\n",
    "print(\"\\nğŸ”— Ajout du casting au dataset principal...\")\n",
    "df_complet = df_complet.merge(acteurs, on='tconst', how='left')\n",
    "df_complet = df_complet.merge(realisateurs, on='tconst', how='left')\n",
    "\n",
    "# Remplacer NaN par listes vides\n",
    "df_complet['acteurs'] = df_complet['acteurs'].apply(lambda x: x if isinstance(x, list) else [])\n",
    "df_complet['realisateurs'] = df_complet['realisateurs'].apply(lambda x: x if isinstance(x, list) else [])\n",
    "\n",
    "print(f\"\\nâœ… Dataset final : {df_complet.shape}\")\n",
    "print(f\"\\nğŸ¬ Exemples de casting :\")\n",
    "\n",
    "# Afficher quelques exemples\n",
    "for idx in range(min(3, len(df_complet))):\n",
    "    row = df_complet.iloc[idx]\n",
    "    if len(row['acteurs']) > 0 or len(row['realisateurs']) > 0:\n",
    "        print(f\"\\n  ğŸ“½ï¸ {row['primaryTitle']} ({int(row['startYear']) if pd.notna(row['startYear']) else '?'})\")\n",
    "        if len(row['acteurs']) > 0:\n",
    "            print(f\"     ğŸ­ Acteurs : {', '.join(row['acteurs'][:3])}\")\n",
    "        if len(row['realisateurs']) > 0:\n",
    "            print(f\"     ğŸ¬ RÃ©alisateur(s) : {', '.join(row['realisateurs'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step6",
   "metadata": {},
   "source": [
    "## ğŸ’¾ Ã‰TAPE 6 : Sauvegarde finale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save_final",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarde finale\n",
    "parquet_final = PARQUET_DIR / \"imdb_complet_avec_cast.parquet\"\n",
    "df_complet.to_parquet(parquet_final, engine='pyarrow', compression='snappy', index=False)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"âœ… TRAITEMENT TERMINÃ‰ !\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nğŸ’¾ Dataset final : {parquet_final}\")\n",
    "print(f\"ğŸ“Š Dimensions : {df_complet.shape}\")\n",
    "print(f\"\\nğŸ“‹ Statistiques :\")\n",
    "print(f\"  â€¢ Films : {len(df_complet):,}\")\n",
    "print(f\"  â€¢ Avec notes : {df_complet['averageRating'].notna().sum():,}\")\n",
    "print(f\"  â€¢ Avec titres franÃ§ais : {df_complet['frenchTitle'].notna().sum():,}\")\n",
    "print(f\"  â€¢ Avec acteurs : {df_complet['acteurs'].apply(len).gt(0).sum():,}\")\n",
    "print(f\"  â€¢ Avec rÃ©alisateurs : {df_complet['realisateurs'].apply(len).gt(0).sum():,}\")\n",
    "\n",
    "print(\"\\nğŸ” AperÃ§u final :\")\n",
    "display(df_complet[['primaryTitle', 'frenchTitle', 'startYear', 'genres', 'averageRating', 'acteurs', 'realisateurs']].head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
